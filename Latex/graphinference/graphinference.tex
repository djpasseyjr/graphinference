\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\title{Graph Inference on Timeseries Data}
\author{D.J. Passey}

\begin{document}
\maketitle

\section{Understanding the Limitations of Vector Autoregression}

    In Stock 2001, the authors point out, "The Taylor [rule] is “backward 
    looking” in the sense that the Fed reacts to past information..., 
    and several researchers have argued that Fed behavior is more
    appropriately described by forward looking behavior."
    Though not addressing autoregression directly, this makes an
    interesting point about how autoregression, and most models are
    "backward looking" while real world processes involve
    anticipating the future.
    The authors go on to explain that the Taylor rule was updated
    to respond to the autoregressive prediction of what inflation
    would be in the near future. It is interesting to note that
    while this new model does incoorperate a notion of predicting
    the future, mathematically, it is still completely backward looking.
    Later, the authors mention the following issues: 
    \begin{enumerate}
        \item "The
        standard methods of statistical inference (such as computing 
        standard errors for impulse responses) may give misleading 
        results if some of the variables are highly persistent [8].
        Another limitation is that, without modification, standard 
        VARs miss nonlinearities, conditional heteroskedasticity, and
        drifts or breaks in parameters."
        \item "While useful as a benchmark, small
        VARs of two or three variables are often unstable and thus poor predictors of the future
        (Stock and Watson [1996])."
        \item "However, adding variables to the VAR creates complications, because the
        number of VAR parameters increases as the square of the number of variables: a ninevariable,
        four-lag VAR has 333 unknown coefficients (including the intercepts).
        Unfortunately, macroeconomic time series data cannot provide reliable estimates of all
        these coefficients without further restrictions."
        \item "a common assumption made in
        structural VARs is that variables like output and inflation
        are sticky and do respond “within the period” to monetary 
        policy shocks. This seems plausible over the period of a 
        single day, but becomes less plausible over a month or
        quarter."
    \end{enumerate}

    \section{Working through the assumptions behind VAR in \textit{Multivariate
    Time Series Analysis} by Tsay}
   
        Tsay defines a multivariate time series as
        \[
            \bm{z}_t = (z_{1t}, \cdots, z_{kt})'
        \]
    
        The problem of interest is defined as predicting $\bm{z}_{T+1}$ based on
        the data $\{\bm{z}_1, \cdots, \bm{z}_T\}$. At this point an abstract
        model, $g(.)$ is employed to solve the problem via
        \[
            \dot{\bm{z}}_{T+1} = g(\bm{z}_T, \bm{z}_{T-1}, \cdots, \bm{z}_1)
        \]
    
        This formulation is great for pedagogical use, but I'll explore
        some of the details that it obscures. Often, the goal is not
        simply to predict $\bm{z}_{T+1}$ but rather to gain
        insight into the dynamics of the data-generating process. Vector
        auto-regression is a powerful tool, not because of it's
        predictive power, but because of the statistical guarantees
        attached to its \textit{explanations} for why the behavior
        occurred. This distinction is important because prediction
        tasks are somewhat model agnostic. That is to say that
        the prediction task doesn't care what kind of model made
        a given prediction, only if it was a better prediction
        than another model.
   
   
        On the other hand, for explanatory tools the details of the model
        are fundamental. Understanding the assumptions, the structure,
        and common failure modes of an explanitory model are vital if 
        one is to interpret results relative to a particular phenomenon.
    
        Another way of framing this problem is by assuming an underlying
        data generating process $f$. It is hard to go much father than this
        without assumptions about $f$. Is it a discrete time system?
        Is it a continuous system? Is it a hybrid? Can it be described
        as a system at all?
    
        Maybe we can start by assuming that the time series
        data of interest belongs to the physical world,
        (since it can be
        measured) and therefore, it occurred because of some general
        system.
    
        Our task is to understand and explain a small piece of
        the process that created the data. Then we can say that
        the data-generating process $f$ can tell us what the next
        data point will be, given the history of all previous data
        points \textit{along with} the complete state of 
        the world.
    
        Then, we model this data generating process by buildig $g$,
        the function that tries to \textit{explain} how the data
        came to be.
    
        For complex systems, an explanitory model is unlikely to
        be able to explain very much. In the social sciences,
        there is so much change in the system of interest, that it might
        be incorrect to assume that the underlying model $g$ is constant.
        But, by making simplifications, we can identify and explain
        certain patterns and processes that occur in the world.
    
        At this point, the model $g$ has no structure. It can be anything.
        Agent based, a lookup table, a differential equation, a
        branching process, a difference equation.
    
        Some models are optimized to fit to the data, and based on 
        the optimization technique used, we can infer patterns and 
        structure in the data generating process based on the parameters.
        
        Other models have far fewer parameters and do not need optimized
        statistical fitting. They offer an explanation by their structure
        rather than through parameters and p-values.
    
        Vector auto regression begins with a choice to restrict $g$
        to a class of dynamical system that is not as expressive as 
        other choices would be.
        Namely,
    
        \begin{equation}
            \dot{\bm{z}}_{T+1} = \bm{\pi}_0 
                + \bm{\pi}_1\bm{z}_T 
                + \bm{\pi}_2\bm{z}_{T-1} 
                + \cdots 
                + \bm{\pi}_T\bm{z}_1
        \end{equation}

        where each $\bm{\pi}_i$ is a $k \times k$ matrix.

        Tsay assumes that the time series follows
        a contimuous multivariate distribution. 

        \subsection{Definition: Weakly Stationary}
            A timeseries is said to be weakly stationary if $E[\bm{z}_t]
            = \bm{mu} \forall t$ and if $\text{Cov}[\bm{z}_t] = \Sigma_{\bm{z}}$
        
        \subsection{Definition: Linear Time Series}
            A k dimensional time series $\bm{z}_t$ is said to be linear
            if
            \begin{equation}
                 \bm{z}_t = \bm{\mu} + \sum_{i=0}^\infty \bm{\psi}_i \bm{a}_{t-i}
            \end{equation}

            where $\bm{\mu}$ is a $k$-dimensional constant vector, 
            $\bm{\psi}_0 = I$, the $k \times k$
            identity matrix, $\bm{\psi}_i$ ($i > 0$) are $k \times k$
            constant matrices and $\{\bm{a}_t\}$ is a sequence of 
            idependent identically distributed random vectors with
            mean zero and positive definite covariance matrix $\Sigma_{\bm{a}}$


        \subsection{Wold Decomposition}
            A stationary, purely stochastic processes $\bm{z}_t$ can
            be written as a linear combination of serially uncorrelated
            processes $\bm{e}_t$.

        \subsection{Result}
            If the coefficient matrices of a linear time series
            satisfy
            \[\sum_{i=0}^\infty ||\bm{\psi}_i|| < \infty \]
            then it is is stationary  with $E[\bm{z}_t] = \bm{\mu}$ and

            \begin{equation}
                \text{Cov}[\bm{z}_t] = \sum_{i=0}^\infty 
                \bm{\psi}_i \Sigma_a \bm{\psi}_i'
            \end{equation}

        \subsection{Invertibility}
            A linear time series is invertible if it can be 
            written as
            \begin{equation}
                \bm{z}_t = \bm{c} + \bm{a}_t 
                + \sum_{j=1}^\infty \bm{\pi}_j \bm{z}_{t-j}
            \end{equation}

    \section{Working through Lutkepohl 2005 New Introduction to Multiple
    Time Series Analysis}

        The Tsay book was a little too underspecified for an applied 
        mathematician so I switched to Lutkepohl.

        \subsection{Definition: Multivariate Stochastic Process}

            Let $(\Omega, \mathcal{F}, \text{Pr})$ be a probability space where 
            $\Omega$ is the event space, 
            $\mathcal{F}$ is a sigma algebra over all subsets of $\Omega$,
            and $\text{Pr}: \mathcal{F} \rightarrow \mathbb{R}$ is a probability 
            measure on $\mathcal{F}$.

            We define a \textit{random variable} to be a real valued
            function $y: \Omega \rightarrow \mathbb{R}$ such that for each 
            $c \in \mathbb{R}$, 
            $A_c = \{\omega \in \Omega | \, y(\omega) \leq c\} \in \mathcal{F}$.
            
            Since $A_c$ belongs to $\mathcal{F}$ we can determine it's proportional
            probability. The
            function $F_y: \mathbb{R} \rightarrow [0,1]$ defined by 
            $F_y(c) = \text{Pr}(A_C)$, is the \textit{distribution function} of $y$.

            A \textit{$K$-dimensional random vector}, or a 
            \textit{vector of $K$ random variables}
            is a function $\bm{y}: \Omega \rightarrow \mathbb{R}^K$ such that
            for each $\bm{c} \in \mathbb{R}^k$, 
            $A_{\bm{c}} = \{\omega | \, y_1(\omega) \leq c_1, ... , y_k(\omega) 
            \leq c_k\} \in \mathcal{F}$. 

            A \textit{discrete stochastic process} is a real valued function, 
            $y: Z \times \Omega \rightarrow \mathbb{R}$ where $Z$ is countable and
            for each $t \in Z$, $y(t, \omega)$ is a random variable.

            A \textit{multivariate stochastic process} is a function
            \[
                \bm{y}: Z \times \Omega \rightarrow \mathbb{R}^K
            \]
            such that for each fixed $t \in Z$, $\bm{y}(t, \omega)$ is a
            $K$-dimensional random vector. For simplicity we will denote
            this as $\bm{y}_t$.

        \subsection{First Two Moments of a Univariate Stochastic Process}

            \begin{align*}
                E(y_t) = \mu_t \\
                E[(y_t - \mu_t)^2] \\
                E[(y_t - \mu_t)(y_s - \mu_s)]
            \end{align*}

        \subsection{Data Generating Process}

            By specifying $\omega = \omega_0$ we can study a particular
            realization of $\bm{y}_t(t, \omega)$. It can therefore be 
            thought of as a function $\bm{y}(t, \omega_0): 
            Z \rightarrow \mathbb{R}^K$. The underlying stochastic process 
            is said to have generated the multiple time series. It is
            sometimes called the \textit{data generating process} (DGP).

        \subsection{Vector Autoregressive Process}

            A univariate autoregressive process is a stochastic process
            \[
                y_t = \nu + \alpha_1 y_{t-1} + \cdots + \alpha_p y_{t - p} + u_t
            \]
            where $\nu$ and each $\alpha_i$ are constant real numbers and $y_t$
            ... $y_{t-p}$ and $u_t$ are scalar random variables with $u_t$ denoting the
            prediction error $u_t := y_t - \hat{y}_t$ and $u_t$ uncorrelated 
            with $u_s$ for all $s \neq t$.

            Similarly, a multivariate autoregressive process is a stochastic process
            described by the following equation:
            \[
                \bm{y}_t = \bm{\nu} + A_1 \bm{y}_{t-1} + \cdots + A_p \bm{y}_{t-p} + \bm{u}_t
            \]

            Here, we assume that the $\bm{u}_t$ are identically distributed, zero mean,
            length $K$ random vectors.

        \subsection{VAR($p$) Process} \label{sec:VARp}

            The VAR model of order $p$ or VAR($p$) is the following object
            \[
                \bm{y}_t = \bm{\nu} + A_1 \bm{y}_{t-1} + \cdots + A_p \bm{y}_{t-p} + \bm{u}_t
            \]

            where $t \in \mathbb{Z}$, $\bm{y}$ is a length $K$ random vector,
            $\bm{\nu}$ is a fixed length $K$ vector, the $A_i$ are $K \times K$
            constant coefficient matrices and $\bm{u}_t$ is a white noise
            or innovation process with $E(\bm{u}_t) = \bm{0}$, 
            $E(\bm{u}_t \bm{u}_t^T) = \Sigma_{\bm{u}}$ with 
            $\Sigma_{\bm{u}}$ assumed non-singular, and 
            $E(\bm{u}_t \bm{u}_s^T) = \bm{0}$ when $s \neq t$.

            Any $\text{VAR}(p)$ process can be rewritten as a $\text{VAR}(1)$
            process with the transformation,
            \[
                Y_t = \begin{bmatrix}y_t \\ y_{t-1} \\ \vdots  \\ y_{t - p} \end{bmatrix},
                \quad
                \bm{\nu} = \begin{bmatrix} \nu \\ 0 \\ \vdots \\ 0 \end{bmatrix}, 
                \quad
                U_t = \begin{bmatrix} \bm{u}_t \\ 0 \\ \vdots \\ 0 \end{bmatrix}
            \]
            and
            \[
                \bm{A} = \begin{bmatrix}
                   A_1 & A_2 & \cdots & A_{p-1} & A_p \\
                   I & & & & \\
                   & I & & & \\
                   & & \ddots & & \\
                   & & & I & \\
                \end{bmatrix}
            \]
            
            Thus the following proofs are general:

            \subsection{Result}

            If the eigenvalues of $\bm{A}$ are less than one, a 
            VAR($p$) can be written in the following form:
            \[
                Y_t = \bm{\mu} + \sum_{i=0}^\infty \bm{A}^i\bm{u}_{t-i}
            \]

            \subsection{Proof}

            As show in \ref{sec:VARp}, Any VAR($p$) process can be written
            as a VAR(1) process of the form

            \[ 
                Y_t = \bm{\nu} + \bm{A} Y_{t - 1} + U_t
            \]

            Applying this recursively gives
            \begin{align}
                Y_t = \bm{\nu} + \bm{A}(\bm{\nu} + \bm{A} Y_{t-2} + U_{t-1}) + U_t \\
                Y_t = \bm{\nu} + \bm{A}\bm{\nu} + \bm{A}^2 Y_{t-2} + \bm{A} U_{t-1} + U_t \\
                Y_t = \bm{\nu} + \bm{A}\bm{\nu} + \bm{A}^2 (\bm{\nu} + \bm{A}Y_{t-3} + U_{t-2}) + \bm{A} U_{t-1} + U_t \\
                Y_t = (I + \bm{A} + \bm{A}^2 + \cdots + \bm{A}^m) \bm{\nu} + \bm{A}^{m+1} Y_{t - (m+1)} + \sum_{i = 0} \bm{A}^i \, U_{t - i}.
            \end{align}

            Since the eigenvalues of $\bm{A}$ are all less than one, the series 
            \[ 
                \sum_{i=0}^\infty \bm{A}^i = (I - \bm{A})^{-1} 
            \]

            Similarly, $\bm{A}^m \rightarrow 0$ as $m \rightarrow \infty$.

            Therefore, if we take the limit of the sequence above as $m$
            goes to infinity we obtain

            \[
                Y_t = \bm{\mu} + \sum_{i=1}^{\infty}\bm{A}^i U_{t-i}
            \]

            Where $\bm{\mu} = (I - \bm{A})^{-1} \bm{\nu}$.

        \subsubsection{Discussion}

        This means that a realization of a vector autoregressive
        process can be described as repeated draws from the same
        multivariate normal distribution (if we assume the white noise
        is gaussian, which I think we must for the statistical ananlysis
        to work.)

        We take these repeated draws and multiply them by appropriate
        powers of $\bm{A}$. This causes the prevalance of a particular
        random vector increase or decrease given the magnitude of the 
        entries of $\bm{A}^i$.

        We could approximate this by truncating the infinite series
        at $M$ terms where $||\bm{A}^M||$ is sufficiently small.
        Then we generate a sequence of random $M$ vectors and apply 
        powers of $\bm{A}$ appropriately. To get the next $Y_t$, we shift
        the sequence by one and generate a new random vector to fill in 
        the open spot.
        

            

\end{document}