---
title: "Exploring and Simulating With a Multilevel Model"
output: pdf_document
author: "DJ Passey"
date: "Jan 4, 2023"
header-includes:
     - \renewcommand{\familydefault}{\sfdefault}
---

```{r, setup}
library(renv)
library(here)
# Activate R virtual environment
# The `here()` function should locate the top level
# directory of the enclosing git repository called `graphinference`
proj_dir = here()
print(cat("Double check that project directory\n",
          "is named graphinference:\n", 
          "proj_dir =", proj_dir, "\n"))

renv::activate(proj_dir)

library(lme4)
```

# Dataset: `InstEval`

InstEval is a dataset of how students rate lectures. The dataset was created
with the goal of identifying the most liked instructor. The columns are:

* `s` a factor with levels 1:2972 denoting individual students.

* `d` a factor with 1128 levels from 1:2160, denoting individual professors or lecturers.

* `studage` an ordered factor with levels 2 < 4 < 6 < 8, denoting student's “age” measured in the semester number the student has been enrolled.

* `lectage` an ordered factor with 6 levels, 1 < 2 < ... < 6, measuring how many semesters back the lecture rated had taken place.

* `service` a binary factor with levels 0 and 1; a lecture is a “service”, if held for a different department than the lecturer's main one.

* `dept` a factor with 14 levels from 1:15, using a random code for the department of the lecture.

* `y` a numeric vector of ratings of lectures by the students, using the discrete scale 1:5, with meanings of ‘poor’ to ‘very good’.

Each observation is one student's rating for a specific lecture (of one lecturer, during one semester in the past).


If our goal is to find the best liked lecturer, we are interested in the relative sizes of the effect of `d`
on `y`. The best liked lecturer should have the biggest positive effect on `y`.

However, we would like to include in the regression the fact that some
students may give mainly positive ratings while others may give more negative ones. 
Similarly, students who are older may give different types of ratings. We can control
for this by including a random intercept for each student and a random intercept
for student age.

Similarly, some departments may get higher ratings because their subjects are
well liked. We can control for this so that the effect of the professor is
department agnostic by including a random intercept for department.

There is a column for service denoting if a professor lectured in their main
department or a different one. It is possible that this column has interactions with department
and professor, that is, for some departments, service may improve your rating, and for others,
service may decrease your rating. Similarly, there may be professors who do better outside
their department, and other professors who don't.

My inclanation is to control for this with a random intercept on `service:d`

Check that instructor (`d` column) is nested in `dept`. In other words,
each instructor belongs to a single department.
```{r}
all(rowSums(xtabs(~ d + dept, InstEval) != 0L ) == 1)
```


Check that each student is associated with exactly one studage. (Otherwise we
would have ratings from students across multiple years.)
```{r}
all(rowSums(xtabs(~ s + studage, InstEval) != 0L ) == 1)
```

## Regressions

```{r}
instr_eval1 <- lm(y ~ d, data=InstEval)
summary(instr_eval1)
```

```{r}
instr_eval2 <- lm(y ~ service + dept + d, data=InstEval)
summary(instr_eval2)
```
```{r}
instr_eval3 <- lm(y ~ service + dept + d + service:d, data=InstEval)
summary(instr_eval3)
```
```{r}
anova(instr_eval2, instr_eval3)
```

## Mixed Effect Models
```{r}
me_instr_eval1 <- lmer(y ~ service + dept + d + d:service + (1 | studage / s), data=InstEval)
summary(me_instr_eval1)
```

```{r}
anova(me_instr_eval1)
```

```{r}
me_instr_eval2 <- lmer(y ~ service + dept + d + d:service + (1 | s), data=InstEval)
summary(me_instr_eval2)
```

```{r}
anova(me_instr_eval2, me_instr_eval1)

```

In conclusion, nesting students in student age doesn't really help.

When you look at the effect size and polarity of d12 in each of the models above,
it varies wildly. This bothered me at first: shouldn't at least the direction of the
effect be consistent? But then I realized that the regressions aren't giving us
inconsistent information, they are just saying, if this is your model of how
things work, this is the effect we see.

For example, when we don't include department, d12 cannot be differentiated from
the mean score. However, when we include department and service in the regression, she
appears to have a highly significant, highly positive effect on her rating. However,
when we include dept:service, she suddenly has a highly *negative* effect on ratings.
That is to say that her mix of department and service schedule explains her high ratings.

By which measure should she be evaluated? Which one is the *truth*? The answer is that
none of them are true, they are simply metrics that fall out of how you choose to
explain lecture ratings.

## Mixed Effect Model Data Simulation

Our previous models were a little too complicated for data simulation so we will
train a simple mixed effect model:

```{r}
simple_me_reg <- lmer(y ~ service + d + (1 | dept), data=InstEval)
summary(simple_me_reg)
```

```{r}

```
A mixed effect model can be written in two ways. First, in this form:
$$
y_i = X \beta + Z \alpha + \epsilon
$$